{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity and Regression Analysis\n",
    "In this tutorial, we will again be using the spatial dataset of county-level election and demographic statistics for the United States. This time, we'll explore different methods to diagnose and account for multicollinearity in our data. Specifically, we'll calculate variance inflation factor (VIF), and compare parameter estimates and model fit in a multivariate regression predicting 2016 county voting preferences using an OLS model, a ridge regression, a lasso regression, and an elastic net regression.\n",
    "\n",
    "Objectives:\n",
    "* ***Calculate a variance inflation factor to diagnose multicollinearity.***\n",
    "* ***Use geographicall weighted regression to identify if the multicollinearity is scale dependent.***\n",
    "* ***Interpret model summary statistics.***\n",
    "* ***Describe how multicollinearity impacts stability in parameter esimates.***\n",
    "* ***Explain the variance/bias tradeoff and describe how to use it to improve models***\n",
    "* ***Draw a conclusion based on contrasting models.***\n",
    "\n",
    "Review: \n",
    "* [Dormann, C. et al. (2013). Collinearity: a review of methods to deal with it and a simulation study evaluating their performance. Ecography, 36(1), 27-46.](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2012.07348.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysal as ps\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from libpysal.weights.contiguity import Queen\n",
    "import libpysal\n",
    "from splot.esda import plot_moran\n",
    "from esda.moran import Moran\n",
    "from statsmodels.api import OLS\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to load the 'Elections' dataset from the libpysal library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libpysal.examples import load_example\n",
    "elections = load_example('Elections')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click on this url to learn more about the variables in this dataset: https://geodacenter.github.io/data-and-lab//county_election_2012_2016-variables/\n",
    "\n",
    "As you can see, there are a lot of data values available in this dataset. Let's say we want to learn more about what county-level factors influence percent change in democratic vote between 2012 and 2016 (pct_pt_16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's see what files are available in the 'Elections' data example\n",
    "elections.get_file_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in elections shapefile as \"votes\"\n",
    "votes = gpd.read_file(elections.get_path('election.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data description on the link above, you see that this is an exceptionally large dataset with many variables. During lecture, we discussed how there are two types of multicollinearity in our data:\n",
    "\n",
    "*Intrinsic multicollinearity:* is an artifact of how we make observations. Often our measurements serve as proxies for some latent process (for example, we can measure percent silt, percent sand, and percent clay as proxies for the latent variable of soil texture). There will be slight variability in the information content between each proxy measurement, but they will not be independent of one another.\n",
    "\n",
    "*Incidental collinearity:* is an artifact of how we sample complex populations. If we collect data from a subsample of the landscape where we don't see all combinations of our predictor variables (do not have good cross replication across our variables). We often induce collinearity in our data just because we are limitted in our ability to sample the environment at the scale of temporal/spatial variability of our process of interest. Incidental collinearity is a model formulation problem.(See here for more info on how to avoid it)[https://people.umass.edu/sdestef/NRC%20601/StudyDesignConcepts.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 1: Looking at the data description, pick two variables that you believe will be intrinsically multicollinear. List and describe these variables. Why did you pick them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to formulate our predictive/diagnostic model. We want to create a subset of the \"votes\" geopandas data frame that contains ten predictor variables and our response variable (pct_pt_16) two variables you selected under TASK 1. First, create a list of the variables you'd like to select.\n",
    "\n",
    "### TASK 2: Type out the column names of your selected predictor variables in my_list. Make sure you use the two variables selected under TASK 1, and eight additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_list = [\"pct_pt_16\", <list your variables here>]\n",
    "\n",
    "#For example:\n",
    "my_list = [\"pct_pt_16\", #The number in diff_2016 expressed as a percent of the total votes. Negative if fewer votes were cast for the Democratic candidate\n",
    "           \"PST120214\", #Population, percent change - April 1, 2010 to July 1, 2014\n",
    "           \"SEX255214\", #Female persons, percent, 2014\n",
    "           \"RHI125214\", #White alone, percent, 2014\n",
    "           \"RHI225214\", #Black or African American alone, percent, 2014\n",
    "           \"EDU685213\", #Bachelor’s degree or higher, percent of persons age 25+, 2009-2013\n",
    "           \"VET605213\", #Veterans, 2009-2013\n",
    "           \"HSG495213\", #Median value of owner-occupied housing units, 2009-2013\n",
    "           \"HSD310213\", #Persons per household, 2009-2013\n",
    "           \"INC910213\", #Per capita money income in past 12 months (2013 dollars), 2009-2013\n",
    "           \"LND110210\"] #Land area in square miles, 2010\n",
    "           \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure all your columns are there:\n",
    "votes[my_list].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot matrix\n",
    "We call the process of getting to know your data (ranges and distributions of the data, as well as any relationships between variables) \"exploratory data analysis\". Pairwise plots of your variables, called scatterplots, can provide a lot of insight into the type of relationships you have between variables. A scatterplot matrix is a pairwise comparison of all variables in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use seaborn.pairplot to plot a scatterplot matrix of you 10 variable subset:\n",
    "sns.pairplot(votes[my_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 3:\n",
    "Answer: Do you observe any collinearity in this dataset? How would you describe the relationship between your two \"incidentally collinear\" variables that you selected based on looking at variable descriptions? \n",
    "\n",
    "### TASK 4:\n",
    "Answer: What is plotted on the diagonal of the scatterplot matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing collinearity globally:\n",
    "During class, we discussed the Variance Inflation Factor, which describes the magnitude of variance inflation that can be expected in an OLS parameter estimate for a given variable *given pairwise collinearity between that variable and another variable*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF = 1/(1-R2) of a pairwise OLS regression between two predictor variables\n",
    "#We can use a built-in function \"variance_inflation_factor\" from statsmodel.api to calculate VIF\n",
    "#Learn more about the function\n",
    "?variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate VIFs on our dataset\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(votes[my_list[1:10]].values, i) for i in range(votes[my_list[1:10]].shape[1])]\n",
    "vif[\"features\"] = votes[my_list[1:10]].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collinearity is always present. When is it a problem?\n",
    "Generally speaking, VIF > 10 are considered \"too much\" collinearity. But this value is somewhat arbitrary: the extent to which variance inflation will impact your analysis is highly context dependent. There are two primary contexts where variance inflation is problematic:\n",
    "\n",
    " 1\\. **You are using your analysis to evaluate variable importance:** If you are using parameter estimates from your model to diagnose which observations have physically important relationships with your response variable, variance inflation can make an important predictor look unimportant, and parameter estimates will be highly leveraged by small changes in the data. \n",
    "\n",
    " 2\\. **You want to use your model to make predictions in a situation where the specific structure of collinearity between variables may have shifted:** When training a model on collinear data, the model only applies to data with that exact structure of collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caluculate a linear regression on the global data:\n",
    "In this next step, we're going to calculate a linear regression in our data an determine whether there is a statistically significant relationship between per capita income and percent change in democratic vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, forumalate the model. See weather_trend.py in \"Git_101\" for a refresher on how.\n",
    "\n",
    "#extract variable that you want to use to \"predict\"\n",
    "X = np.array(votes[my_list[1:10]].values)\n",
    "#standardize data to assist in interpretation of coefficients\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "#extract variable that we want to \"predict\"\n",
    "Y = np.array(votes['pct_dem_16'].values)\n",
    "#standardize data to assist in interpretation of coefficients\n",
    "Y = (Y - np.mean(X)) / np.std(Y)\n",
    "\n",
    "lm = OLS(Y,X)\n",
    "lm_results = OLS(Y,X).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(lm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5: Answer: which coefficients indicate a statisticall significant relationship between parameter and pct_pt_16?\n",
    "\n",
    "*Type answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5: Answer: Are any of these parameters subject to variance inflation?\n",
    "\n",
    "*Type answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot our residuals to see if there are any spatial patterns in them.\n",
    "\n",
    "Remember residuals = predicted - fitted values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add model residuals to our \"votes\" geopandas dataframe:\n",
    "votes['lm_resid']=OLS(Y,X).fit().resid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, in OLS regression we depend out our residuals being normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(votes['lm_resid'].values, shade=True, color='slategrey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the residuals in space to see if there are clear patterns:\n",
    "votes.plot('lm_resid', linewidth=.05, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we did with out spatial autocorrelation analysis, we can evaluate whether spatial autocorrelation has impacted our results is if we see spatial autocorrelation in the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weights matrix\n",
    "weights = Queen.from_dataframe(votes)\n",
    "\n",
    "# calculate Moran's I and plot\n",
    "moran = Moran(votes['lm_resid'], w=weights)\n",
    "plot_moran(moran, zstandard=True, figsize=(10,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized regression: ridge penalty\n",
    "In penalized regression, we intentionally bias the parameter estimates to stabilize them given collinearity in the dataset.\n",
    "\n",
    "From https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\n",
    "\"As mentioned before, ridge regression performs ‘L2 regularization‘, i.e. it adds a factor of sum of squares of coefficients in the optimization objective. Thus, ridge regression optimizes the following:\n",
    "\n",
    "**Objective = RSS + α * (sum of square of coefficients)**\n",
    "\n",
    "Here, α (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients. α can take various values:\n",
    "\n",
    "* **α = 0:** The objective becomes same as simple linear regression. We’ll get the same coefficients as simple linear regression.\n",
    "\n",
    "* **α = ∞:** The coefficients will approach zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.\n",
    "\n",
    "* **0 < α < ∞:** The magnitude of α will decide the weightage given to different parts of objective. The coefficients will be somewhere between 0 and ones for simple linear regression.\"\n",
    "\n",
    "In other words, the ridge penalty shrinks coefficients such that collinear coefficients will have more similar coefficient values. It has a \"grouping\" tendency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when L2=0, Ridge equals OLS\n",
    "model = Ridge(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "#force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)\n",
    "#Print out the model coefficients\n",
    "print(model.coef_)\n",
    "#How do these compare with OLS coefficients above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when L2 approaches infinity, coefficients become nearly zero, and MAE equals the variance of our response variable:\n",
    "model = Ridge(alpha=10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "#force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)\n",
    "#Print out the model coefficients\n",
    "print(model.coef_)\n",
    "#How do these compare with OLS coefficients above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized regression: lasso penalty\n",
    "\n",
    "From https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\n",
    "\"LASSO stands for Least Absolute Shrinkage and Selection Operator. I know it doesn’t give much of an idea but there are 2 key words here – ‘absolute‘ and ‘selection‘.\n",
    "\n",
    "Lets consider the former first and worry about the latter later.\n",
    "\n",
    "Lasso regression performs L1 regularization, i.e. it adds a factor of sum of absolute value of coefficients in the optimization objective. Thus, lasso regression optimizes the following:\n",
    "\n",
    "**Objective = RSS + α * (sum of absolute value of coefficients)**\n",
    "Here, α (alpha) works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients. Like that of ridge, α can take various values. Lets iterate it here briefly:\n",
    "\n",
    "* **α = 0:** Same coefficients as simple linear regression\n",
    "* **α = ∞:** All coefficients zero (same logic as before)\n",
    "* **0 < α < ∞:** coefficients between 0 and that of simple linear regression\n",
    "\n",
    "Yes its appearing to be very similar to Ridge till now. But just hang on with me and you’ll know the difference by the time we finish.\"\n",
    "\n",
    "In other words, the lasso penalty shrinks unimportant coefficients down towards zero, automatically \"selecting\" important predictor variables. But what if that shrunken coefficient is induced by incidental collinearity (i.e. is a feature of how we sampled our data)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when L1=0, Lasso equals OLS\n",
    "model = Lasso(alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "#force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)\n",
    "#Print out the model coefficients\n",
    "print(model.coef_)\n",
    "#How do these compare with OLS coefficients above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when L1 approaches infinity, certain coefficients will become exactly zero, and MAE equals the variance of our response variable:\n",
    "model = Lasso(alpha=10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "#force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)\n",
    "#Print out the model coefficients\n",
    "print(model.coef_)\n",
    "#How do these compare with OLS coefficients above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized regression: elastic net penalty\n",
    "\n",
    "In other words, the lasso penalty shrinks unimportant coefficients down towards zero, automatically \"selecting\" important predictor variables. The ridge penalty shrinks coefficients of collinear predictor variables nearer to each other, effectively partitioning the magnitude of response from the response variable between them, instead of \"arbitrarily\" partitioning it to one group.\n",
    "\n",
    "We can also run a regression with a linear combination of ridge and lasso, called the elastic net, that has a cool property called \"group selection.\"\n",
    "\n",
    "The ridge penalty still works to distribute response variance equally between members of \"groups\" of collinear predictor variables. The lasso penalty still works to shrink certain coefficients to exactly zero so they can be ignored in model formulation. The elastic net produces models that are both sparse and stable under collinearity, by shrinking parameters of members of unimportant collinear predictor variables to exactly zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when L1 approaches infinity, certain coefficients will become exactly zero, and MAE equals the variance of our response variable:\n",
    "model = ElasticNet(alpha=1, l1_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "#force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)\n",
    "#Print out the model coefficients\n",
    "print(model.coef_)\n",
    "#How do these compare with OLS coefficients above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 6: Match these elastic net coefficients up with your original data. Do you see a logical grouping(s) between variables that have non-zero coefficients?Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra credit: By variable L1 and L2 penalties, we modify parameter estimates and MSE. How would you go about selecting appropriate values of L1 and/or L2 for your regression? Show it with code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
